{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rejecting bad objects and duplicates\n",
    "\n",
    "In this notebook I compose the final catalog of LSBGs, by rejecting the bad objects (found by the group of people who did the visual inspection) and duplicates,  found by checking objects in our catalog that lie in a distance less than 2\" between each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import some libraries we are going to need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the full catalog now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "Galfit_res = fits.open('y3_gold_2_2_lsbg_galfit_v3.2.fits')\n",
    "#Galfit_res[1].header.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their photometric and structural properties now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coadd_object_id = Galfit_res[1].data[\"COADD_OBJECT_ID\"]\n",
    "object_num = Galfit_res[1].data[\"OBJECT_NUMBER\"] #what is this?\n",
    "# Coordinates\n",
    "RA = Galfit_res[1].data[\"RA\"]\n",
    "DEC = Galfit_res[1].data[\"DEC\"]\n",
    "# A, B image\n",
    "A_IMAGE = Galfit_res[1].data[\"A_IMAGE\"]\n",
    "B_IMAGE = Galfit_res[1].data[\"B_IMAGE\"]\n",
    "# Effective radius \n",
    "R_eff = 0.263*Galfit_res[1].data[\"RE_G\"]\n",
    "R_eff_err = 0.263*Galfit_res[1].data[\"RE_ERR_G\"]\n",
    "# Sersic index\n",
    "n_ser = Galfit_res[1].data[\"N\"]\n",
    "n_ser_err = Galfit_res[1].data[\"N_ERR\"]\n",
    "# Magnitudes\n",
    "mag_g = Galfit_res[1].data[\"MAG_G\"]\n",
    "mag_r = Galfit_res[1].data[\"MAG_R\"]\n",
    "mag_i = Galfit_res[1].data[\"MAG_I\"]\n",
    "# Errors on magnitudes \n",
    "mag_g_err = Galfit_res[1].data[\"MAG_ERR_G\"]\n",
    "mag_r_err = Galfit_res[1].data[\"MAG_ERR_R\"]\n",
    "mag_i_err = Galfit_res[1].data[\"MAG_ERR_I\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the Coadds of the bad objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad = pd.read_csv(\"Coadd_bad.csv\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the coadd_ids of the \"bad\" objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "bad_coadd = df_bad['Coadd_id'].values\n",
    "print(len(bad_coadd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a dataframe that contains the coadd_ids of all objects. That way we can easily exclude the bad `coadd_ids` from the list of ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame({'all_coadds':np.asarray(coadd_object_id)})\n",
    "df_all=pd.DataFrame({'all_coadds':np.array(coadd_object_id).byteswap().newbyteorder()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21324\n"
     ]
    }
   ],
   "source": [
    "mask = df_all['all_coadds'].isin(bad_coadd)\n",
    "df_new = df_all[~df_all.all_coadds.isin(df_bad.Coadd_id)]\n",
    "print(len(df_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can drop all properties (and not just the coadds) of those objects that have coadd_ids belonging in the bad coadd_id list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "\n",
    "table = Table.read('y3_gold_2_2_lsbg_galfit_v3.2.fits')\n",
    "pandas_df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21324\n"
     ]
    }
   ],
   "source": [
    "df_new = pandas_df[~pandas_df.COADD_OBJECT_ID.isin(df_bad.Coadd_id)]\n",
    "print(len(df_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's exclude duplicates and objects that are very close to each other.\n",
    "\n",
    "First, exclude the duplicate coadds (we have three of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21321\n"
     ]
    }
   ],
   "source": [
    "Coadds = df_new['COADD_OBJECT_ID'].values\n",
    "mask_dupl = df_new.duplicated(subset='COADD_OBJECT_ID', keep='first') # Create a mask\n",
    "# Now keep only the uniques\n",
    "df_unique = df_new[~mask_dupl]\n",
    "#df_unique.head()\n",
    "print(len(df_unique['RA'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get COADD_IDs, RAs and DECs of the  unique objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coadds = df_unique['COADD_OBJECT_ID'].values\n",
    "RAs = df_unique['RA'].values\n",
    "DECs = df_unique['DEC'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_dupls = [] #empty array, keeps the separations of duplicates\n",
    "Coadds_dupl = [] #Empty array, keeps the Coadds_of the duplicates\n",
    "RAs_dupl = [] #Empty array, keeps the RAs of the duplicates\n",
    "DECs_dupl = [] #Empty array, keeps the DECs of the duplicates\n",
    "\n",
    "n_size = len(RAs)\n",
    "max_sep = 4. # Maximum allowed separation in arcsec\n",
    "\n",
    "# Define the full catalog here\n",
    "C_full = SkyCoord(ra=RAs*u.degree, dec=DECs*u.degree, frame='icrs')\n",
    "\n",
    "for i in range(n_size):\n",
    "    C_loc = SkyCoord(ra=RAs[i]*u.degree, dec=DECs[i]*u.degree, frame='icrs')\n",
    "    separ = C_full.separation(C_loc).arcsec #Separations in arcsec\n",
    "    \n",
    "    # ===========================================================\n",
    "    # ===========================================================\n",
    "    # Keep those with separation less than max_sep\n",
    "    sep_with = separ[(separ<max_sep)&(separ>0.0)].tolist()\n",
    "    if (sep_with!=[]): # If it is not empty\n",
    "        Coadds_dup_loc = Coadds[(separ<max_sep)&(separ>0.0)]\n",
    "        RAs_dup_loc = RAs[(separ<max_sep)&(separ>0.0)]\n",
    "        DECs_dup_loc = DECs[(separ<max_sep)&(separ>0.0)]\n",
    "        \n",
    "        Coadds_dupl = np.concatenate((Coadds_dupl,Coadds_dup_loc))\n",
    "        RAs_dupl = np.concatenate((RAs_dupl,RAs_dup_loc))\n",
    "        DECs_dupl = np.concatenate((DECs_dupl,DECs_dup_loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get cutouts of all the duplicates to check which of them to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#print(Coadds_dupl.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_cuts = len(DECs_dupl)\n",
    "#zoom = 15\n",
    "# ==================================\n",
    "# ==================================\n",
    "#for i in range(N_cuts):\n",
    "    # Give a name to the figure. Name them as \"Image_cand_(i).jpb\n",
    "    # Where i is the number of the candidate\n",
    "    # This is easy to change to ra, dec or coadd ID or whatever...\n",
    "#    fig_name = \"Image_cand_{0}.jpg\".format(i)\n",
    "    \n",
    "    #Create now the name of the URL\n",
    "    # This need to have as inputs (that change) the RA, DEC of each objec and zoom\n",
    "#    RA_loc = RAs_dupl[i] #The RA of the i-th object\n",
    "#    DEC_loc = DECs_dupl[i] # The DEC of the i-th object\n",
    "#    Coadd_loc = Coadds_dupl[i]\n",
    "#    Coadd_loc = int(Coadd_loc)\n",
    "#    url_name = \"https://data.darkenergysurvey.org/fnalmisc/lsbgalfit/v3/segmap_{0}.png\".format(Coadd_loc)\n",
    "    \n",
    "#    urllib.urlretrieve(url_name, fig_name) #Retreaves and saves each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now display the bad objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_cols = 3\n",
    "#n_rows = int(N_cuts/n_cols)+1\n",
    "\n",
    "#fig,ax = plt.subplots(n_rows,n_cols, figsize=(16,3*n_rows))\n",
    "\n",
    "#for i in range(N_cuts):\n",
    "#    fig_name = \"Image_cand_{0}.jpg\".format(i)\n",
    "#    image = Image.open(fig_name)\n",
    "#    ax[i//3][i%3].imshow(image)\n",
    "#    ax[i//3][i%3].set_title(\"{0}\".format(i), fontsize=18)\n",
    "#    ax[i//3][i%3].set_xticks([]) # Remove x ticks\n",
    "#    ax[i//3][i%3].set_yticks([]) # Remove y ticks\n",
    "    \n",
    "    \n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array with the indices of the \"bad\" coadd_id duplicates\n",
    "Bad_IDs = [2,3,5,7,9,11,14,15,17,20,22,24,26,29,30,33,34,\n",
    "          37,38,39,42,45,46,48,52,54,56,58,60,63,64,66,\n",
    "          68,70,73]\n",
    "\n",
    "Bad_IDs = np.asarray(Bad_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \n",
    "Bad_dupls_coadd = Coadds_dupl[Bad_IDs]\n",
    "# Make it integer\n",
    "Bad_coadd_dupls = Bad_dupls_coadd.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe where we exclude the bad objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21286\n"
     ]
    }
   ],
   "source": [
    "df_final = df_unique[~df_unique.COADD_OBJECT_ID.isin(Bad_coadd_dupls)]\n",
    "print(len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as csv\n",
    "df_final.to_csv(\"Galfit_final_IDs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a csv file that contains the coadd ids of the duplicate objects that are in the catalog\n",
    "Good_IDs = np.asarray([0,1,4,8,10,12,13,16,18,19,21,23,25,27,28,31,32,35,36,\n",
    "           40,41,43,44,47,49,50,51,53,55,57,59,61,62,65,67,69,71,72])\n",
    "\n",
    "Good_dupl_coadds = Coadds_dupl[Good_IDs]\n",
    "Good_dupl_coadds = Good_dupl_coadds.astype(int)\n",
    "# Create a data frame\n",
    "df_dupls = pd.DataFrame({'Coadd_IDs_dupls':np.asarray(Good_dupl_coadds).byteswap().newbyteorder()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as csv\n",
    "df_dupls.to_csv(\"Dupls_coadd_IDs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_final = Table.from_pandas(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.write('test_table.fits')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
